import json
from typing import List

import pickle
import numpy as np
from tqdm import tqdm
from sklearn import utils
from tensorflow.keras.utils import to_categorical
from gensim.models.doc2vec import Doc2Vec, TaggedDocument


def train_opcode_embeddings(
        tagged_opcodes_list: List[TaggedDocument],
        model_path: str,
        dm: int = 1,
        dm_mean: int = 1,
        workers: int = 4,
        max_epochs: int = 100,
        min_count: int = 1,
        window_size: int = 10,
        dimensions: int = 256,
        alpha: float = 0.015,
        negative: int = 5
    ) -> Doc2Vec:

    model: Doc2Vec = Doc2Vec(
        dm=dm,
        dm_mean=dm_mean,
        workers=workers,
        min_count=min_count,
        window=window_size,
        vector_size=dimensions,
        alpha=alpha,
        min_alpha=alpha,
        negative=negative
    )

    model.build_vocab(tagged_opcodes_list)

    for epoch in range(max_epochs):
        print('iteration {0}'.format(epoch))
        model.train(utils.shuffle([x for x in tqdm(tagged_opcodes_list)]),
                    total_examples=len(tagged_opcodes_list),
                    epochs=1)
        
        # decrease the learning rate
        model.alpha -= 0.0002
        
        # fix the learning rate, no decay
        model.min_alpha = model.alpha

    model.save(model_path)
    print("Model Saved")

    return model

def get_vectors(model: Doc2Vec, corpus_size: int, vectors_size: int = 256, vectors_type: str = "train"):
    """
    Get vectors from trained doc2vec model
    :param doc2vec_model: Trained Doc2Vec model
    :param corpus_size: Size of the data
    :param vectors_size: Size of the embedding vectors
    :param vectors_type: Training or Testing vectors
    :return: list of vectors
    """
    vectors = np.zeros((corpus_size, vectors_size))
    for i in range(0, corpus_size):
        prefix = vectors_type + '_' + str(i)
        vectors[i] = model.dv[prefix]
    return vectors

malware_full_data_path = "./malware_data_with_slices.json"
model_path = "./d2v_model.model"

tagged_opcodes_list: List[TaggedDocument] = []

x_train_data: List[TaggedDocument] = []
y_train_data: List[int] = []

x_test_data: List[TaggedDocument] = []
y_test_data: List[int] = []

with open(malware_full_data_path, "r") as jsonfile:
    opcode_data_json = json.loads(jsonfile.read())

for dataset_type, opcode_data_dict in opcode_data_json.items():
    idx = 0
    for malware_type, file_opcode_dict in opcode_data_dict.items():
        for filename, opcode_data in file_opcode_dict.items():
            if opcode_data != []:
                tagged_opcode = TaggedDocument(words=opcode_data, tags=[f"{dataset_type}_{idx}"])
                idx += 1
                label = 1 if malware_type == 'malware' else 0
                if dataset_type == 'train':
                    x_train_data.append(tagged_opcode)
                    y_train_data.append(label)
                else:
                    x_test_data.append(tagged_opcode)
                    y_test_data.append(label)
                
tagged_opcodes_list = x_train_data + x_test_data

print(len(x_train_data), len(y_train_data))
print(f"Train 0: {y_train_data.count(0)}, 1: {y_train_data.count(1)}")
print(len(x_test_data), len(y_test_data))
print(f"Test 0: {y_test_data.count(0)}, 1: {y_test_data.count(1)}")

print(len(tagged_opcodes_list))

model = train_opcode_embeddings(tagged_opcodes_list, model_path)

X_train = get_vectors(model, len(x_train_data), 256, 'train')
pickle.dump(X_train, open("X_train.npy", "wb"))

X_test = get_vectors(model, len(x_test_data), 256, 'test')
pickle.dump(X_test, open("X_test.npy", "wb"))

y_train = to_categorical(y_train_data)
pickle.dump(y_train, open("y_train.npy", "wb"))

y_test = to_categorical(y_test_data)
pickle.dump(y_test, open("y_test.npy", "wb"))

print(len(X_train), len(y_train))
print(f"Train 1: {y_train.argmax(axis=1).sum()}")
print(len(X_test), len(y_test))
print(f"Test  1: {y_test.argmax(axis=1).sum()}")