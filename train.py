# https://www.tensorflow.org/tutorials/keras/keras_tuner
# MLP, CNN, LSTM
import pickle
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Dropout, Input, LSTM, \
    BatchNormalization, GlobalAveragePooling2D, Reshape, Bidirectional
from tensorflow.keras import regularizers
import keras_tuner as kt

X_test = pickle.load(open("X_test.npy", "rb"))
y_test = pickle.load(open("y_test.npy", "rb"))
X_train = pickle.load(open("X_train.npy", "rb"))
y_train = pickle.load(open("y_train.npy", "rb"))

def create_mlp_model(hp, feature_channels=256, dropout=0.4, activation="relu"):

    model = Sequential()
    model.add(Input(shape=(feature_channels)))

    hp_num_layers = hp.Choice('num_layers', values=[1, 2, 3])
    hp_units = hp.Choice('units', values=[32, 64, 128, 256])
    for _ in range(hp_num_layers):
        model.add(Dropout(dropout))
        model.add(Dense(hp_units, activation=activation))
        model.add(BatchNormalization())

    model.add(Dense(16, activation=activation))
    model.add(Dense(2, activation='softmax'))
    
    
    # Tune the learning rate for the optimizer
    # Choose an optimal value from 0.01, 0.001, or 0.0001
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
        loss=keras.losses.CategoricalCrossentropy(),
        metrics=['accuracy']
    )
    return model

def create_cnn_model(hp, feature_channels=256, dropout=0.3, activation="relu"):

    model = Sequential()
    model.add(Input(shape=(feature_channels)))
    model.add(Reshape((16, 16)))

    hp_num_layers = hp.Choice('num_layers', values=[1, 2, 3])
    hp_conv_channels = hp.Choice('conv_channels', values=[16, 32, 64, 128])
    for _ in range(hp_num_layers):
        model.add(Conv2D(hp_conv_channels, (3, 3), strides=1, padding='same', activation=activation,
                            kernel_regularizer=regularizers.l1_l2(5e-4, 5e-4),
                            kernel_initializer=tf.keras.initializers.LecunNormal()))
        model.add(BatchNormalization())
        model.add(Dropout(dropout))

    model.add(GlobalAveragePooling2D())
    model.add(Dense(2, activation='softmax'))
    
    # Tune the learning rate for the optimizer
    # Choose an optimal value from 0.01, 0.001, or 0.0001
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-3, 1e-4])
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
        loss=keras.losses.CategoricalCrossentropy(),
        metrics=['accuracy']
    )
    return model

def create_lstm_model(hp, feature_channels=256, dropout=0.2, activation="relu"):

    model = Sequential()
    model.add(Input(shape=(feature_channels)))
    model.add(Reshape((16, 16)))

    hp_lstm_channels = hp.Choice('lstm_channels', values=[64, 128, 256])

    model.add(LSTM(hp_lstm_channels,input_shape=((16, 16)),activation='relu',return_sequences=True))
    model.add(Dropout(dropout))

    model.add(LSTM(hp_lstm_channels // 2,activation=activation))
    model.add(Dropout(dropout))

    model.add(Dense(32, activation=activation))
    model.add(Dropout(dropout))
    model.add(Dense(2, activation='softmax'))
    
    # Tune the learning rate for the optimizer
    # Choose an optimal value from 0.01, 0.001, or 0.0001
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-3, 1e-4])
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
        loss=keras.losses.CategoricalCrossentropy(),
        metrics=['accuracy']
    )
    return model

def create_bidirection_lstm_model(hp, feature_channels=256, dropout=0.2, activation="relu"):

    model = Sequential()
    model.add(Input(shape=(feature_channels)))
    model.add(Reshape((16, 16)))

    hp_lstm_channels = hp.Choice('lstm_channels', values=[64, 128, 256])

    model.add(Bidirectional(LSTM(hp_lstm_channels,input_shape=((16, 16)),activation='relu',return_sequences=True)))
    model.add(Dropout(dropout))

    model.add(Bidirectional(LSTM(hp_lstm_channels // 2,activation=activation)))
    model.add(Dropout(dropout))

    model.add(Dense(32, activation=activation))
    model.add(Dropout(dropout))
    model.add(Dense(2, activation='softmax'))
    
    # Tune the learning rate for the optimizer
    # Choose an optimal value from 0.01, 0.001, or 0.0001
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-3, 1e-4])
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
        loss=keras.losses.CategoricalCrossentropy(),
        metrics=['accuracy']
    )
    return model

def search_hyper_parameters_mlp_model():
    tuner = kt.Hyperband(create_mlp_model,
                        objective='val_accuracy',
                        max_epochs=20,
                        factor=3,
                        directory='results',
                        project_name='malware-classify-mlp')

    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)

    tuner.search(x = X_train, y = y_train,
            validation_data=[X_test, y_test], epochs=50, callbacks=[stop_early], batch_size=32)

    # Get the optimal hyperparameters
    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

    print(f"""
    The hyperparameter search is complete. The optimal number MLP layers are {best_hps.get('num_layers')}, number of units in the densely-connected
    layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
    is {best_hps.get('learning_rate')}.
    """)

def search_hyper_parameters_cnn_model():
    tuner = kt.Hyperband(create_cnn_model,
                        objective='val_accuracy',
                        max_epochs=20,
                        factor=3,
                        directory='results',
                        project_name='malware-classify-cnn')

    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)

    tuner.search(x = X_train, y = y_train,
            validation_data=[X_test, y_test], epochs=50, callbacks=[stop_early], batch_size=32)

    # Get the optimal hyperparameters
    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

    print(f"""
    The hyperparameter search is complete. The optimal number MLP layers are {best_hps.get('num_layers')}, number of conv channels in the convolution layer is {best_hps.get('conv_channels')} and the optimal learning rate for the optimizer
    is {best_hps.get('learning_rate')}.
    """)

def search_hyper_parameters_lstm_model():
    tuner = kt.Hyperband(create_lstm_model,
                        objective='val_accuracy',
                        max_epochs=20,
                        factor=3,
                        directory='results',
                        project_name='malware-classify-lstm')

    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)

    tuner.search(x = X_train, y = y_train,
            validation_data=[X_test, y_test], epochs=50, callbacks=[stop_early], batch_size=32)

    # Get the optimal hyperparameters
    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

    # print(f"""
    # The hyperparameter search is complete. The optimal number MLP layers are {best_hps.get('num_layers')}, number of lstm channels in the lstm layer is {best_hps.get('lstm_channels')} and the optimal learning rate for the optimizer
    # is {best_hps.get('learning_rate')}.
    # """)

def search_hyper_parameters_bidirection_lstm_model():
    tuner = kt.Hyperband(create_bidirection_lstm_model,
                        objective='val_accuracy',
                        max_epochs=20,
                        factor=3,
                        directory='results',
                        project_name='malware-classify-bidirection-lstm')

    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)

    tuner.search(x = X_train, y = y_train,
            validation_data=[X_test, y_test], epochs=50, callbacks=[stop_early], batch_size=32)

    # Get the optimal hyperparameters
    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

    # print(f"""
    # The hyperparameter search is complete. The optimal number MLP layers are {best_hps.get('num_layers')}, number of lstm channels in the lstm layer is {best_hps.get('lstm_channels')} and the optimal learning rate for the optimizer
    # is {best_hps.get('learning_rate')}.
    # """)

if __name__ == '__main__':
    # print(y_train)
    # search_hyper_parameters_mlp_model()
    # search_hyper_parameters_cnn_model()
    search_hyper_parameters_lstm_model()
    # search_hyper_parameters_bidirection_lstm_model()